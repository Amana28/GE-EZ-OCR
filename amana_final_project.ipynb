{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ML to improve the recognition of Handwritten Ge'ez Characters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Motivation:\n",
    "\n",
    "This project's motivation is advancing handwritten image recognition for Ge'ez (Ethiopic) script. The Ge'ez script is used as an alphabet system by more than 20 Afro-Asiatic and Nilo-Saharan languages in Ethiopia and Eritrea and has more than 50 million users. It also serves as the script for Amharic, the official language of the Federal Government of Ethiopia. The Ge'ez script is also valuable because it makes Ethiopia the only country in africa with its own unique alphabet system. Because of this and becuase of its large user base, the Ge'ez script needs highly reliable Handwriting recognition software for different purposes like preserving ancient historical documents (digital transcription), securing bank check processing, fast postal address sorting, and so on. This project aims to replicate (and if possible improve) the accuracy of existing handwritten Ethiopic alphabet recognition programs by using a combination of several neural network algorithms (CNN and RNN).\n",
    "\n",
    "## - Paper used for reference\n",
    "\n",
    "<a href=\"https://link.springer.com/article/10.1007/s42452-021-04742-x#Bib1\" target=\"_blank\">AHWR-Net: offline handwritten amharic word recognition using convolutional recurrent neural network</a>\n",
    "\n",
    "\n",
    "## - Dataset Used\n",
    "\n",
    "In this project I will be using the <a href=\"https://sites.google.com/view/hawdb-v1?pli=1\" target=\"_blank\">HARD-1 dataset</a>, a publicly available dataset prepared for handwritten Ge’ez letter recognition collected using Amharic native speakers and writers. The dataset contains 33, 672 handwritten and labeled Amharic word images (12, 064 original handwritten images and 21,608 augmented images from the originals by randomly applying functions such as rotation, shifting, shrinking, expanding, degrading, and applying a varying amount of Gaussian noise and blurring). The images are grey scale images of size 32 X 128 pixels with only one channel. The images in the HARD-1 training and testing Data sets have been manually labeled (numbered) using the table below. Each input image contains no more than 11 Ge’ez characters (letters, numbers, punctuation etc..). \n",
    "\n",
    "#### Table: Ge'ez characters used for this project \n",
    "![Table](images/ethiopic_numbering.png)\n",
    "\n",
    "\n",
    "## - Final Prgress of Project\n",
    "\n",
    "### Importing Datasets\n",
    "\n",
    "- Successusfully imported the training, validation and testing datasets \n",
    "- Displayed some of their features like thier number of dimensions, shape and size\n",
    "\n",
    "### Visualizing inputs\n",
    "\n",
    "- Generated random numbers to select several random inputs from the training dataset X_train\n",
    "- Since my input data was stored in a numpy array, I was able to convert them into png files using cv2 and display them with their corresponding labels\n",
    "- Verified labels using the above table\n",
    "\n",
    "### Created DataLoader objects \n",
    "\n",
    "- Created three DataLoader objects from existing datasets using batch size 64\n",
    "- Tried to replicate the DataLoader objects used by the CNN model in our textbook\n",
    "\n",
    "### Building a CNN model\n",
    "\n",
    "- To make my training process go faster I build a CNN model with 4 convoltional laters\n",
    "- Input size is [64, 1, 32, 128] dimensions corresponding to [batch_size, channels, W, H]\n",
    "- To get optimal output I converted the input to a tensor object of size [64, 512, 1, 32] \n",
    "    - This gives me a 512, 1 x 32 tensor objects\n",
    "\n",
    "### Preparing CNN output for CNN input \n",
    "\n",
    "- Converted feature map into a sequence of size 512 and length 32 by doing column wise concatenation for each sample\n",
    "- [64, 512, 1, 32] tensor object converted to a sequence of size [64, 32, 512]\n",
    "- Took me a long time to understand how to convert the output of my CNN model to sth that can be used by a BI-LSTM model\n",
    "\n",
    "### Building a Bi-LSTM model\n",
    "\n",
    "- Created a Bidirectional Gated Reccurrent Unit (BI-LSTM) RNN model that has 2 layers, hidden unit size of 512, and dropout at each layer\n",
    "\n",
    "\n",
    "### Creating a combined Model\n",
    "\n",
    "- Despite my best efforts, I was not able to replicate the findings of the paper I referenced without running into numerous issues\n",
    "    - Problems faced\n",
    "        - I found the process of converting the feature map output from my CNN model into a sequential data really challenging \n",
    "        - I had trouble merging my two Neural Network models into a single model that can be trained using a single loss function for parameter tuning \n",
    "        - Difficulty interpreting the feature extraction and sequnce encoding process used by the reaserchers used in the study \n",
    "        (there is no guide with regard to how they build their models -- they only mention what they were. Because of this I found myself spending a lot of time debuggin code trying to replicate their findings rather than making meaningful progress on my project) \n",
    "\n",
    "\n",
    "\n",
    "### Results and Conclusion\n",
    "\n",
    "- Because of the numerous problems I faced builing my program I was not able to make any meaningful conclusions with regard to the aim of my project\n",
    "(well outside of building several neural network models from scratch is really hard)\n",
    "- On the otherhand I have leaned a lot about image processing using Machine learning models and I hope to use what I have learned to continually build on this project.\n",
    "- Despite the challenges I faced with this project, this project has widened (exponentially!) my knowledge in the field of visual/image processing\n",
    "\n",
    "\n",
    "\n",
    "### Future Work \n",
    "\n",
    "- Since I have put a lot of time and energy into this project and also becuase I am interested in the potential outcome of this project, I plan on uploading my code to github and continutually working on this project until I manage to create a progarm that I does what I hoped to acheive (improve the recognition of Handwritten Ge'ez Characters). \n",
    "- Hopefully someday I will manage to build a program that improves upon the work of the paper I sited (I plan on contacting professor Lee Spector if I manage to do so)!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing HARD-1 (Handwritten Amharic text Recognition Dataset) Training, Validaing and Testing Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train ( ndim: 4  shape: (9622, 32, 128, 1)  size: 39411712 )\n",
      "y_train ( ndim: 2  shape: (9622, 11)  size: 105842 ) \n",
      "\n",
      "X_val ( ndim: 4  shape: (1202, 32, 128, 1)  size: 4923392 )\n",
      "y_val ( ndim: 2  shape: (1202, 11)  size: 13222 ) \n",
      "\n",
      "X_test ( ndim: 4  shape: (1200, 32, 128, 1)  size: 4915200 )\n",
      "y_test ( ndim: 2  shape: (1200, 11)  size: 13200 )\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.load(\"HARD-1/X_train.npy\")\n",
    "y_train = np.load(\"HARD-1/y_train.npy\")\n",
    "X_val = np.load(\"HARD-1/X_val.npy\")\n",
    "y_val = np.load(\"HARD-1/y_val.npy\")\n",
    "X_test = np.load(\"HARD-1/X_test.npy\")\n",
    "y_test = np.load(\"HARD-1/y_test.npy\")\n",
    "\n",
    "print(\"X_train (\", \"ndim:\", X_train.ndim, \" shape:\", X_train.shape, \" size:\", X_train.size , \")\")\n",
    "print(\"y_train (\", \"ndim:\", y_train.ndim, \" shape:\", y_train.shape, \" size:\", y_train.size, \") \\n\")\n",
    "print(\"X_val (\", \"ndim:\", X_val.ndim, \" shape:\", X_val.shape, \" size:\", X_val.size , \")\")\n",
    "print(\"y_val (\", \"ndim:\", y_val.ndim, \" shape:\", y_val.shape, \" size:\", y_val.size, \") \\n\")\n",
    "print(\"X_test (\", \"ndim:\", X_test.ndim, \" shape:\", X_test.shape, \" size:\", X_test.size , \")\")\n",
    "print(\"y_test (\", \"ndim:\", y_test.ndim, \" shape:\", y_test.shape, \" size:\", y_test.size, \")\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the input and labels of our dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from numpy.random import seed\n",
    "from numpy.random import randint\n",
    "# seed random number generator\n",
    "seed(1)\n",
    "# generate three integers\n",
    "rand_numbers = randint(0, 9622, 3)\n",
    "\n",
    "# use random numbers to get random words in our dataset \n",
    "rand_word1 = X_train[rand_numbers[0]]\n",
    "rand_word2 = X_train[rand_numbers[1]]\n",
    "rand_word3 = X_train[rand_numbers[2]]\n",
    "\n",
    "\n",
    "# convert to the proper data type to be used by cv2\n",
    "rand_word1_img = rand_word1 * 255\n",
    "rand_word1_img = rand_word1_img.astype(np.uint8)\n",
    "rand_word2_img = rand_word2 * 255\n",
    "rand_word2_img = rand_word2_img.astype(np.uint8)\n",
    "rand_word3_img = rand_word3 * 255\n",
    "rand_word3_img = rand_word3_img.astype(np.uint8)\n",
    "\n",
    "# convert numpy arrays to an images using the imwirte module\n",
    "# images used in the description section of this project \n",
    "# import cv2\n",
    "# cv2.imwrite(\"images/rand_amharic_word1.png\", rand_word1_img)\n",
    "# cv2.imwrite(\"images/rand_amharic_word2.png\", rand_word2_img)\n",
    "# cv2.imwrite(\"images/rand_amharic_word3.png\", rand_word3_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fig: Sample image of 3 random inputs (X) with their corresponding labels (y)\n",
    "\n",
    "Word1 = ![Image1](images/rand_amharic_word1.png)\n",
    "\n",
    "Word2 = ![Image2](images/rand_amharic_word2.png)\n",
    "\n",
    "Word3 = ![Image3](images/rand_amharic_word3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word1 Label: [150  89 188  43 109  91 114 300 300 300 300]\n",
      "Word2 Label: [125 245 191 188  67 300 300 300 300 300 300]\n",
      "Word3 Label: [272 300 300 300 300 300 300 300 300 300 300]\n"
     ]
    }
   ],
   "source": [
    "print(\"Word1 Label:\", y_train[rand_numbers[0]])\n",
    "print(\"Word2 Label:\", y_train[rand_numbers[1]])\n",
    "print(\"Word3 Label:\", y_train[rand_numbers[2]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Varibales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "# device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataLoader objects out of training, validation and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_torch = torch.tensor(X_train.transpose(0, 3, 1, 2))\n",
    "y_train_torch = torch.tensor(y_train.reshape(9622,1,11))\n",
    "X_val_torch = torch.tensor(X_val.transpose(0, 3, 1, 2))\n",
    "y_val_torch = torch.tensor(y_val.reshape(1202,1,11))\n",
    "X_test_torch = torch.tensor(X_test.transpose(0, 3, 1, 2))\n",
    "y_test_torch = torch.tensor(y_test.reshape(1200,1,11))\n",
    "\n",
    "# Create custom dataset class\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = MyDataset(X_train_torch, y_train_torch)\n",
    "val_dataset = MyDataset(X_val_torch, y_val_torch)\n",
    "test_dataset = MyDataset(X_test_torch, y_test_torch)\n",
    "\n",
    "\n",
    "# Create dataloader objects\n",
    "train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl= torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulding a CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 128])\n",
      "torch.Size([64, 64, 32, 128])\n",
      "torch.Size([64, 64, 16, 64])\n",
      "torch.Size([64, 128, 16, 64])\n",
      "torch.Size([64, 128, 8, 32])\n",
      "torch.Size([64, 256, 8, 32])\n",
      "torch.Size([64, 256, 4, 32])\n",
      "torch.Size([64, 256, 2, 32])\n",
      "torch.Size([64, 512, 2, 32])\n",
      "torch.Size([64, 512, 1, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "model = nn.Sequential()\n",
    "x = torch.ones((64, 1, 32, 128)) \n",
    "print(model(x).shape)\n",
    "\n",
    "model.add_module('Conv1', nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1))\n",
    "model.add_module('BN1', nn.BatchNorm2d(64))\n",
    "model.add_module('ReLU1', nn.ReLU())\n",
    "print(model(x).shape)\n",
    "\n",
    "model.add_module('pool1', nn.MaxPool2d(kernel_size=2))\n",
    "print(model(x).shape)\n",
    "\n",
    "model.add_module('Conv2', nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1))\n",
    "model.add_module('BN2', nn.BatchNorm2d(128))\n",
    "model.add_module('ReLU2', nn.ReLU())\n",
    "print(model(x).shape)\n",
    "\n",
    "model.add_module('pool2', nn.MaxPool2d(kernel_size=2))\n",
    "print(model(x).shape)\n",
    "\n",
    "model.add_module('Conv3', nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1))\n",
    "model.add_module('BN3', nn.BatchNorm2d(256))\n",
    "model.add_module('ReLU3', nn.ReLU())\n",
    "print(model(x).shape)\n",
    "\n",
    "model.add_module('pool3', nn.MaxPool2d((2, 1)))\n",
    "print(model(x).shape)\n",
    "\n",
    "model.add_module('pool4', nn.MaxPool2d((2, 1)))\n",
    "print(model(x).shape)\n",
    "\n",
    "model.add_module('Conv4', nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1))\n",
    "model.add_module('BN4', nn.BatchNorm2d(512))\n",
    "model.add_module('ReLU4', nn.ReLU())\n",
    "print(model(x).shape)\n",
    "\n",
    "model.add_module('pool5', nn.MaxPool2d((2, 1)))\n",
    "print(model(x).shape)\n",
    "\n",
    "feature_map = model(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting CNN output to RNN input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size = 64\n",
      "torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "# Convert feature map into a sequence of size 32 X 512 by doing column-wise concatenation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Initialize empty list to store sequential features\n",
    "sequential_features = []\n",
    "# For each batch iterate over each column in the feature map\n",
    "for batch in range(feature_map.shape[0]):\n",
    "    tens0 = feature_map[0][0][0]\n",
    "    tens1 = feature_map[0][1][0]\n",
    "    stacked = torch.stack((tens0, tens1), -1)\n",
    "    for col in range(2, 511, 2):\n",
    "        tens0 = feature_map[0][col][0]\n",
    "        tens1 = feature_map[0][col+1][0]\n",
    "        combined2 = torch.stack((tens0, tens1), -1)\n",
    "        stacked = torch.cat((stacked, combined2), -1)\n",
    "    sequential_features.append(stacked)\n",
    "    \n",
    "print(f'Batch Size = {len(sequential_features)}')\n",
    "print(sequential_features[0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Bidirectional LSTM model with dropout and 512 hidden cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BILSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):\n",
    "        super(BILSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)  # 2 for bidirection\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)  # 2 for bidirection \n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(self.dropout(out[:, -1, :]))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (Conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (BN1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (ReLU1): ReLU()\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (Conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (BN2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (ReLU2): ReLU()\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (Conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (BN3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (ReLU3): ReLU()\n",
      "  (pool3): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool4): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (Conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (BN4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (ReLU4): ReLU()\n",
      "  (pool5): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "my_lstm = BILSTM(64, 512, 2, 11, 0.5)\n",
    "print(model)\n",
    "# result = my_lstm(torch.randn(64, 32, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BIGRU(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size):\n",
    "#         super().__init__()\n",
    "#         # self.rnn = nn.RNN(input_size, hidden_size, num_layers=2, batch_first=True)\n",
    "#         self.rnn = nn.GRU(input_size, hidden_size, num_layers = 2, batch_first=True, bidirectional=True)\n",
    "#         # self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, 1) \n",
    "#     def forward(self, x):\n",
    "#         _, hidden = self.rnn(x)\n",
    "#         out = hidden[-1, :, :] # we use the final hidden state\n",
    "#         # from the last hidden layer as\n",
    "#         # the input to the fully connected\n",
    "#         # layer\n",
    "#         out = self.fc(out)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the LSTM model\n",
    "lstm_model = BILSTM(512, 512, 2, 11, 0.5)\n",
    "z = torch.ones((64, 512, 1, 32)) \n",
    "#lstm_model(z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiailzing loss and optimizor functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function that trains our model using both the CNN and RNN I built and a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, train_dl, valid_dl):\n",
    "    loss_hist_train = [0] * num_epochs \n",
    "    accuracy_hist_train = [0] * num_epochs\n",
    "    loss_hist_valid = [0] * num_epochs\n",
    "    accuracy_hist_valid = [0] * num_epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_dl:\n",
    "            pred = model(x_batch.float())\n",
    "            loss = loss_fn(pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss_hist_train[epoch] += loss.item()*y_batch.size(0)\n",
    "            is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "            accuracy_hist_train[epoch] += is_correct.sum()\n",
    "        loss_hist_train[epoch] /= len(train_dl.dataset)\n",
    "        accuracy_hist_train[epoch] /= len(train_dl.dataset) \n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in val_dl:\n",
    "                pred = model(x_batch)\n",
    "                loss = loss_fn(pred, y_batch)\n",
    "                loss_hist_valid[epoch] += loss.item() * y_batch.size(0)\n",
    "                is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "                accuracy_hist_valid[epoch] += is_correct.sum()\n",
    "        loss_hist_valid[epoch] /= len(val_dl.dataset)\n",
    "        accuracy_hist_valid[epoch] /= len(val_dl.dataset)\n",
    "\n",
    "        print(f'Epoch {epoch+1} accuracy: '\n",
    "            f'{accuracy_hist_train[epoch]:.4f} val_accuracy: '\n",
    "            f'{accuracy_hist_valid[epoch]:.4f}')\n",
    "    return loss_hist_train, loss_hist_valid, \\\n",
    "         accuracy_hist_train, accuracy_hist_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(1)\n",
    "# num_epochs = 1\n",
    "# hist = train(model, NUM_EPOCHS, train_dl, val_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
